{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter Tuning. Model: XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "# Import warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source data location and data dictionary"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Documentation: http://archive.ics.uci.edu/ml/datasets/concrete+compressive+strength\n",
    "# Data Description: http://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete_Readme.txt\n",
    "# Data Source:  https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete_Data.xls\n",
    "\n",
    "# Data location: https://github.com/socratesk/YHatSchoolOfAI/blob/master/data/concrete.csv\n",
    "# Data Dictionary: https://github.com/socratesk/YHatSchoolOfAI/blob/master/data/breastcancer.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from github/ local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Concrete Data file from local file system\n",
    "# concrete_data = pd.read_csv(\"data/concrete.csv\")\n",
    "\n",
    "# Load Concrete Data file from github\n",
    "concrete_data = pd.read_csv(\"https://raw.githubusercontent.com/socratesk/YHatSchoolOfAI/master/data/concrete.csv\")\n",
    "\n",
    "# Print the shape\n",
    "print (concrete_data.shape)\n",
    "\n",
    "# Print few rows to visualize the data\n",
    "concrete_data.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into Train and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SPLIT_RATIO = 0.2\n",
    "\n",
    "# Split data into Train and Test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(concrete_data.drop([\"compressive_strength\"], axis=1), \n",
    "                                                    concrete_data[\"compressive_strength\"], \n",
    "                                                    test_size = SPLIT_RATIO, \n",
    "                                                    random_state = 230)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us consider XGBoost algorithm to train the model and tune its hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate XGBoost Model\n",
    "model = XGBRegressor(random_state=999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model with Train and Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the XGBoost model using train dataset\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the class using generated XGBOOST model with Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions for test data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print first few predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first 25 predicted values\n",
    "print(y_pred[0:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the RMSE score\n",
    "rmse_initial = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Print RMSE score\n",
    "print (\"RMSE of XGBoost model: \", rmse_initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Parameter Tuning </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune <code>n_estimators</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set range of parameters for n_estimator\n",
    "param_test0 = {\n",
    "    'n_estimators':list(range(100, 200, 5))\n",
    "}\n",
    "\n",
    "# Build the XGBoost model for the range of max_depth and min_child_weight values\n",
    "gridSearch = GridSearchCV(XGBRegressor(learning_rate =0.01, \n",
    "                                   #n_estimators=100, \n",
    "                                   max_depth=5,\n",
    "                                   min_child_weight=2, \n",
    "                                   gamma=0,\n",
    "                                   subsample=0.90, \n",
    "                                   colsample_bytree=0.90,\n",
    "                                   silent=False,\n",
    "                                   random_state=999),\n",
    "             param_grid = param_test0, \n",
    "             scoring = 'r2',\n",
    "             n_jobs = 4,\n",
    "             iid = False,  \n",
    "             return_train_score=True,\n",
    "             cv = 5)\n",
    "\n",
    "# Fit the train dataset\n",
    "gridSearch.fit(X_train, y_train)\n",
    "\n",
    "# Print scores for each parameter.\n",
    "# REMEMBER the scores are based on train dataset only and NOT on test dataset\n",
    "gridSearch.best_params_, gridSearch.best_score_   # gridSearch.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract best scores from Grid search\n",
    "estimatorvalue = gridSearch.best_params_['n_estimators']  # 195"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune <code>max_depth</code> and <code>min_child_weight</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set range of parameters for max_depth and min_child_weight\n",
    "param_test1 = {\n",
    "    'max_depth':list(range(10, 15, 1)),\n",
    "    'min_child_weight':list(range(1, 6, 1))\n",
    "}\n",
    "\n",
    "# Build the XGBoost model for the range of max_depth and min_child_weight values\n",
    "gridSearch = GridSearchCV(XGBRegressor(learning_rate =0.01, \n",
    "                                   n_estimators=estimatorvalue, \n",
    "                                   #max_depth=5,\n",
    "                                   #min_child_weight=2, \n",
    "                                   gamma=0,\n",
    "                                   subsample=0.90, \n",
    "                                   colsample_bytree=0.90,\n",
    "                                   silent=False,\n",
    "                                   random_state=999),\n",
    "             param_grid = param_test1, \n",
    "             scoring = 'r2',\n",
    "             n_jobs = 4,\n",
    "             iid = False,  return_train_score=True,\n",
    "             cv = 5)\n",
    "\n",
    "# Fit the train dataset\n",
    "gridSearch.fit(X_train, y_train)\n",
    "\n",
    "# Print scores for each parameter.\n",
    "# REMEMBER the scores are based on train dataset only and NOT on test dataset\n",
    "gridSearch.best_params_, gridSearch.best_score_   # gridSearch.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract best scores from Grid search\n",
    "maxdepthvalue = gridSearch.best_params_['max_depth']  # 13\n",
    "minchildvalue = gridSearch.best_params_['min_child_weight']  # 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the values of <code>max_depth</code> and <code>min_child_weight</code> from previous step and tune <code>subsample</code> and <code>colsample_bytree</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set range of parameters for subsample and colsample_bytree\n",
    "param_test2 = {\n",
    " 'subsample':[0.7, 0.75, 0.8, 0.85, 0.9, 0.95],\n",
    " 'colsample_bytree':[0.6, 0.65, 0.7, 0.75, 0.8]\n",
    "}\n",
    "\n",
    "# Build the XGBoost model for the range of subsample and colsample_bytree values\n",
    "gridSearch = GridSearchCV(XGBRegressor(learning_rate = 0.01, \n",
    "                                   n_estimators = estimatorvalue, \n",
    "                                   max_depth = maxdepthvalue,\n",
    "                                   min_child_weight = minchildvalue, \n",
    "                                   gamma = 0,\n",
    "                                   #subsample=0.75, \n",
    "                                   #colsample_bytree=0.72,\n",
    "                                   random_state=999\n",
    "                                       ),\n",
    "             param_grid = param_test2, \n",
    "             scoring = 'r2',\n",
    "             n_jobs = 4,\n",
    "             iid = False, \n",
    "             cv = 5)\n",
    "\n",
    "# Fit the train dataset\n",
    "gridSearch.fit(X_train, y_train)\n",
    "\n",
    "# Print scores for each parameter.\n",
    "# REMEMBER the scores are based on train dataset only and NOT on test dataset\n",
    "gridSearch.best_params_, gridSearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract best scores from Grid search\n",
    "colsamplevalue = gridSearch.best_params_['colsample_bytree'] # 0.75\n",
    "subsamplevalue = gridSearch.best_params_['subsample'] # 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the values of <code>max_depth</code>, <code>min_child_weight</code>, <code>subsample</code>, and <code>colsample_bytree</code> from previous steps and tune <code>learning_rate</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set range of parameters for learning_rate\n",
    "param_test3 = {\n",
    "    #'learning_rate':[0.3, 0.2, 0.1, 0.01, 0.02, 0.03, 0.001, 0.002, 0.003]\n",
    "    #'learning_rate':[0.08, 0.09, 0.1, 0.11, 0.12]\n",
    "    'learning_rate':[0.085, 0.09, 0.095]\n",
    "}\n",
    "\n",
    "# Build the XGBoost model for the range of gamma values\n",
    "gridSearch = GridSearchCV(XGBRegressor(#learning_rate = 0.01, \n",
    "                                   n_estimators = estimatorvalue, \n",
    "                                   max_depth = maxdepthvalue,\n",
    "                                   min_child_weight = minchildvalue, \n",
    "                                   gamma=0,\n",
    "                                   subsample = subsamplevalue, \n",
    "                                   colsample_bytree = colsamplevalue,\n",
    "                                   random_state=999),\n",
    "             param_grid = param_test3, \n",
    "             scoring = 'r2',\n",
    "             n_jobs = 4,\n",
    "             iid = False, \n",
    "             cv = 5)\n",
    "\n",
    "# Fit the train dataset\n",
    "gridSearch.fit(X_train, y_train)\n",
    "\n",
    "# Print scores for each parameter.\n",
    "# REMEMBER the scores are based on train dataset only and NOT on test dataset\n",
    "gridSearch.best_params_, gridSearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract best scores from Grid search\n",
    "learning_rate = gridSearch.best_params_['learning_rate'] # 0.06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the values of <code>max_depth</code>, <code>min_child_weight</code>, <code>subsample</code>,  <code>colsample_bytree</code>, and <code>gamma</code> from previous steps and tune  <code>reg_alpha</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set range of parameters for reg_alpha\n",
    "param_test4 = {\n",
    " 'reg_alpha':[0.9, 0.95, 1, 1.05, 1.1]\n",
    "}\n",
    "\n",
    "# Build the XGBoost model for the range of reg_alpha values\n",
    "gridSearch = GridSearchCV(XGBRegressor(learning_rate = learning_rate, \n",
    "                                   n_estimators = estimatorvalue, \n",
    "                                   max_depth = maxdepthvalue,\n",
    "                                   min_child_weight = minchildvalue, \n",
    "                                   gamma = 0,\n",
    "                                   subsample = subsamplevalue, \n",
    "                                   colsample_bytree = colsamplevalue,\n",
    "                                   random_state=999),\n",
    "             param_grid = param_test4, \n",
    "             scoring = 'r2',\n",
    "             n_jobs = 4,\n",
    "             iid = False, \n",
    "             cv = 5)\n",
    "\n",
    "# Fit the train dataset\n",
    "gridSearch.fit(X_train, y_train)\n",
    "\n",
    "# Print scores for each parameter.\n",
    "# REMEMBER the scores are based on train dataset only and NOT on test dataset\n",
    "gridSearch.best_params_, gridSearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract best scores from Grid search\n",
    "regalphavalue = gridSearch.best_params_['reg_alpha'] # 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all the tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finalModel = XGBRegressor(\n",
    "                  learning_rate = learning_rate,\n",
    "                  n_estimators = estimatorvalue, \n",
    "                  max_depth = maxdepthvalue,\n",
    "                  min_child_weight= minchildvalue, \n",
    "                  gamma = 0,\n",
    "                  subsample = subsamplevalue, \n",
    "                  colsample_bytree = colsamplevalue,\n",
    "                  reg_alpha = regalphavalue,\n",
    "                  metrics = 'rmse',\n",
    "                  random_state=999,\n",
    "                  silent=True\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With combined parameters train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the final model\n",
    "finalModel.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions for test data\n",
    "y_pred = finalModel.predict(X_test)\n",
    "\n",
    "# Print the predicted values\n",
    "print(y_pred[0:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the model RMSE score after parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the RMSE score\n",
    "rmse_xgb = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Print prediction score\n",
    "print('XGBoost model RMSE score after parameter tuning: %.4f' % rmse_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the initial RMSE score with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print initial RMSE Score\n",
    "print('XGBoost model RMSE score with default parameters: %.4f' % rmse_initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the percentage of decease in RMSE score after parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute percentage improvement\n",
    "perc_improvement = ( (rmse_initial - rmse_xgb) / rmse_initial) * 100\n",
    "\n",
    "# Print percentage improvement\n",
    "print('Percentage of improvement after parameter tuning: %.2f%%' % perc_improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
