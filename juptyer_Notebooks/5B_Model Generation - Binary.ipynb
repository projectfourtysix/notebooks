{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model Generation - Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. [Tutorial links](#tutorials)\n",
    "### 2. [Load CSV File](#csv_file)\n",
    "### 3. [Split the dataset. _Technique: Train-Test split_](#train_test)\n",
    "### 4. [Generate Binary Classification Models using Train-Test split](#binomial_train_test)\n",
    "\n",
    "> #### 4.1. [Logistic Regression Model using Train-Test split](#lr_train_test)\n",
    "#### 4.2. [Random Forest Model using Train-Test split](#rf_train_test)\n",
    "#### 4.3. [k-Nearest Neighbor Model using Train-Test split](#knn_train_test)\n",
    "#### 4.4. [Support Vector Machine (SVM) Model using Train-Test split](#svm_train_test)\n",
    "#### 4.5. [XGBoost Classifier Model using Train-Test split](#xgb_train_test)\n",
    "\n",
    "### 5. [Analyze the model results](#analyze_results)\n",
    "\n",
    "> #### 5.1. [Print Accuracy and AUC of all models](#print_AUC)\n",
    "\n",
    "### 6. [Split the dataset. _Technique: K-Fold Cross Validation_](#k_fold)\n",
    "### 7. [Generate Binary Classification Models using K-Fold split](#binomial_kfold)\n",
    "> #### 7.1. [Logistic Regression Model using K-Fold split](#lr_kfold)\n",
    "#### 7.2. [Random Forest Model using K-Fold split](#rf_kfold)\n",
    "#### 7.3. [k-Nearest Neighbor Model using K-Fold split](#knn_kfold)\n",
    "#### 7.4. [Support Vector Machine (SVM) Model using K-Fold split](#svm_kfold)\n",
    "#### 7.5. [XGBoost Classifier Model using K-Fold split](#xgb_kfold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='tutorials'>1. Tutorial links</a>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Logistic Regression Tutorial/ documentation:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "# Random Forest Classifier Tutorial/ documentation:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    " \n",
    "# k-Nearest Neighbor Tutorial/ documentation:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "    \n",
    "# Support Vector Machine (svm) Tutorial/ documentation:    \n",
    "https://scikit-learn.org/stable/modules/svm.html\n",
    "\n",
    "# Support Vector Machine (svm) Tutorial/ documentation FOR PROBABILITY:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "# XGBoost Tutorial/ documentation:\n",
    "https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn\n",
    "\n",
    "# Evaluation Techniques\n",
    "http://blog.socratesk.com/blog/2018/10/21/validation-strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# Import numpy library\n",
    "import numpy as np\n",
    "\n",
    "# Import Train-Test split library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Import RandomForestClassifier library\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Import KNeighborsClassifier library\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Import Support Vector Machine (SVM) library\n",
    "from sklearn import svm\n",
    "\n",
    "# Import XGBClassifier library\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Import Train-Test split library\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import KFold split library\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Import XGBClassifier library\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Import accuracy score computing library\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Import metrics library\n",
    "from sklearn import metrics\n",
    "\n",
    "# Import matplotlib library\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='csv_file'>2. Load CSV file saved locally after Feature Engineering<a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Loan Data file that is saved after Feature Engineering from local disk\n",
    "loan_data = pd.read_csv(\"LoanData_final.csv\")\n",
    "\n",
    "# Print the shape\n",
    "print (loan_data.shape)\n",
    "\n",
    "# Print few rows to visualize the data\n",
    "loan_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='train_test'>3. Split the dataset. _Technique: Train-Test split_</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Train and Test split ratio to 80:20\n",
    "SPLIT_RATIO = 0.2\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(loan_data.drop('Loan_Status',  axis = 1), \n",
    "                                                    loan_data['Loan_Status'], \n",
    "                                                    test_size=SPLIT_RATIO, \n",
    "                                                    random_state = 21345)\n",
    "\n",
    "# Print the shape of the Train set\n",
    "print(\"Train dataset: \", X_train.shape, Y_train.shape)\n",
    "\n",
    "# Print the shape of the Test set\n",
    "print(\"Test dataset: \", X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='binomial_train_test'>4. Generate Binary Classification Models using Train-Test split</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='lr_train_test'>4.1 Logistic Regression Model using Train-Test split</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Logistic Regression object\n",
    "# liblinear removes warning due to backward compatibility. NOT Important\n",
    "lr_model = LogisticRegression(solver='liblinear', random_state=2011)\n",
    "\n",
    "# Train a Logistic Regression model with Train dataset\n",
    "lr_model.fit(X_train, Y_train)\n",
    "\n",
    "# Predict the outcome\n",
    "y_hat_lr = lr_model.predict(X_test)\n",
    "\n",
    "# Compute the accuracy score and print it\n",
    "accuracy_lr = accuracy_score(Y_test, y_hat_lr)\n",
    "\n",
    "# Print first 8 rows to visualize the prediction.\n",
    "print (\"First few predicted Loan Status values: \", y_hat_lr[:8], \"\\n\")\n",
    "\n",
    "# Compute accuracy score\n",
    "print (\"Accuracy of Logistic Regression model: \", accuracy_lr)\n",
    "\n",
    "# Print confusion matrix of actual and predicted values using Crosstab function\n",
    "pd.crosstab(Y_test, y_hat_lr, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 Print Logistic Regression Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Linear Regression Model\n",
    "lr_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 To output the Binary Probability using Logistic Regression model ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed in one of the classes earlier, the Classifier models accept the Train dataset and generate the model that is capable of predicting \"the probability of the outcome\". <br>\n",
    "\n",
    "In the above example, we used `lr_model.predict(X_test)` to predict the binary outcome. However, if you want a binary probability outcome, do the following -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the binary probability outcome\n",
    "y_hat_lr_proba = lr_model.predict_proba(X_test)\n",
    "\n",
    "# Print first 8 rows to visualize the prediction.\n",
    "y_hat_lr_proba[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 To draw Area Under the Curve (AUC) of Logistic Regression model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute area under the curve (AUC)\n",
    "auc_lr = metrics.roc_auc_score(Y_test, y_hat_lr_proba[:, 1])\n",
    "\n",
    "# Compute False Positive Rate, True Positive Rate, and Thresholds using metrics library\n",
    "fpr, tpr, threshold = metrics.roc_curve(Y_test, y_hat_lr_proba[:, 1])\n",
    "\n",
    "# Set the plotting area/ size\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "# Plot the AUC curve\n",
    "plt.plot(fpr, tpr, label='AUC =' + str(auc_lr))\n",
    "plt.legend(loc=4)\n",
    "\n",
    "# Plot the base line (diagonal dotted line)\n",
    "plt.plot([0, 1], [0, 1], 'k-.', lw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.4 Scatterplot the actual and predicted classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_lr_list=y_hat_lr_proba[:, 1:2].tolist()\n",
    "\n",
    "# Set the plotting area/ size\n",
    "plt.figure(figsize = (12, 6))\n",
    "\n",
    "# Scatter plot the Actual and Predicted values\n",
    "plt.scatter(y_hat_lr_list, Y_test)\n",
    "plt.axvline(x=0.5, c='darkorange')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title(\"Scatter plot of 'Actual' and 'Predicted' values - Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='rf_train_test'>4.2 Random Forest Model using Train-Test split</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Random Forest Classifier object\n",
    "rf_model = RandomForestClassifier(n_estimators=100, min_samples_leaf=1, random_state=2202)\n",
    "\n",
    "# n_estimators - represents no of trees in the forest\n",
    "# n_jobs - No of cores to be used\n",
    "# oob_score - \n",
    "# max_depth - depth of each tree in the forest\n",
    "# min_samples_split - Minimum number of samples required to split an internal node\n",
    "# min_samples_leaf  - Minimum number of samples required to be at a leaf node\n",
    "# max_features - the number of features to consider when looking for best split\n",
    "\n",
    "# Train a Random Forest model with Train dataset\n",
    "rf_model.fit(X_train, Y_train)\n",
    "\n",
    "# Predict the outcome\n",
    "y_hat_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Compute the accuracy score and print it\n",
    "accuracy_rf = accuracy_score(Y_test, y_hat_rf)\n",
    "\n",
    "# Print first 8 rows to visualize the prediction.\n",
    "print (\"First few predicted Loan Status values: \", y_hat_rf[:8], \"\\n\")\n",
    "\n",
    "# Compute accuracy score\n",
    "print (\"Accuracy of Random Forest model: \", accuracy_rf)\n",
    "\n",
    "# Print confusion matrix of actual and predicted values using Crosstab function\n",
    "pd.crosstab(Y_test, y_hat_rf, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Print Random Forest Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Random Forest Model\n",
    "rf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 To output the Binary Probability using Random Forest model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the binary probability outcome\n",
    "y_hat_rf_proba = rf_model.predict_proba(X_test)\n",
    "\n",
    "# Print first 8 rows to visualize the prediction.\n",
    "y_hat_rf_proba[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 To draw Area Under the Curve (AUC) of Random Forest model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute area under the curve (AUC)\n",
    "auc_rf = metrics.roc_auc_score(Y_test, y_hat_rf_proba[:, 1])\n",
    "\n",
    "# Compute False Positive Rate, True Positive Rate, and Thresholds using metrics library\n",
    "fpr, tpr, threshold = metrics.roc_curve(Y_test, y_hat_rf_proba[:, 1])\n",
    "\n",
    "# Set the plotting area/ size\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "# Plot the AUC curve\n",
    "plt.plot(fpr, tpr, label='AUC =' + str(auc_rf))\n",
    "plt.legend(loc=4)\n",
    "\n",
    "# Plot the base line (diagonal dotted line)\n",
    "plt.plot([0, 1], [0, 1], 'k-.', lw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='knn_train_test'>4.3 k-Nearest Neighbor Model using Train-Test split</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a k-Nearest Neighbor object\n",
    "knn_model = KNeighborsClassifier(n_neighbors = 9)\n",
    "\n",
    "# Train a k-Nearest Neighbor model with Train dataset\n",
    "knn_model.fit(X_train, Y_train)\n",
    "\n",
    "# Predict the outcome\n",
    "y_hat_knn = knn_model.predict(X_test)\n",
    "\n",
    "# Compute the accuracy score and print it\n",
    "accuracy_knn = accuracy_score(Y_test, y_hat_knn)\n",
    "\n",
    "# Print first 8 rows to visualize the prediction.\n",
    "print (\"First few predicted Loan Status values: \", y_hat_knn[:8], \"\\n\")\n",
    "\n",
    "# Compute accuracy score\n",
    "print (\"Accuracy of k-Nearest Neighbor model: \", accuracy_knn)\n",
    "\n",
    "# Print confusion matrix of actual and predicted values using Crosstab function\n",
    "pd.crosstab(Y_test, y_hat_knn, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 Print k-Nearest Neighbor Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the k-Nearest Neighbor Model\n",
    "knn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 To output the Binary Probability using k-Nearest Neighbor model ..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<< HOME WORK >>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict the binary probability outcome\n",
    "y_hat_knn_proba =  << your code goes here >>\n",
    "\n",
    "# Print first 8 rows to visualize the prediction.\n",
    "y_hat_knn_proba[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3 To draw Area Under the Curve (AUC) of k-Nearest Neighbor model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<< HOME WORK: your code goes here >>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='svm_train_test'>4.4 Support Vector Machine (SVM)  Model using Train-Test split</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate a Support Vector Machine (SVM) object\n",
    "svm_model = svm.SVC(gamma=0.05, degree=5, kernel='linear')\n",
    "\n",
    "# Train a Support Vector Machine model with Train dataset\n",
    "svm_model.fit(X_train, Y_train)\n",
    "\n",
    "# Predict the outcome\n",
    "y_hat_svm = svm_model.predict(X_test)\n",
    "\n",
    "# Compute the accuracy score and print it\n",
    "accuracy_svm = accuracy_score(Y_test, y_hat_svm)\n",
    "\n",
    "# Print first 8 rows to visualize the prediction.\n",
    "print (\"First few predicted Loan Status values: \", y_hat_svm[:8], \"\\n\")\n",
    "\n",
    "# Compute accuracy score\n",
    "print (\"Accuracy of Support Vector Machine model: \", accuracy_svm)\n",
    "\n",
    "# Print confusion matrix of actual and predicted values using Crosstab function\n",
    "pd.crosstab(Y_test, y_hat_svm, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1 Print Support Vector Machine (SVM) Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the Support Vector Machine Model\n",
    "svm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2 To output the Binary Probability using Support Vector Machine (SVM) model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate a Support Vector Machine (SVM) object to determine probability of binary outcome\n",
    "svm_model_proba = svm.SVC(gamma=0.05, degree=5, kernel='linear', probability=True)\n",
    "\n",
    "# Train a Support Vector Machine model with Train dataset\n",
    "svm_model_proba.fit(X_train, Y_train)\n",
    "\n",
    "# Predict the probability outcome\n",
    "y_hat_svm_proba = svm_model_proba.predict_proba(X_test)\n",
    "\n",
    "# Print first 8 rows to visualize the prediction.\n",
    "y_hat_svm_proba[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.3 To draw Area Under the Curve (AUC) of Support Vector Machine (SVM) model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute area under the curve (AUC)\n",
    "auc_svm = metrics.roc_auc_score(Y_test, y_hat_svm_proba[:, 1])\n",
    "\n",
    "# Compute False Positive Rate, True Positive Rate, and Thresholds using metrics library\n",
    "fpr, tpr, threshold = metrics.roc_curve(Y_test, y_hat_svm_proba[:, 1])\n",
    "\n",
    "# Set the plotting area/ size\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "# Plot the AUC curve\n",
    "plt.plot(fpr, tpr, label='AUC =' + str(auc_svm))\n",
    "plt.legend(loc=4)\n",
    "\n",
    "# Plot the base line (diagonal dotted line)\n",
    "plt.plot([0, 1], [0, 1], 'k-.', lw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='xgb_train_test'>4.5 XGBoost Classifier Model using Train-Test split</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate a XGBoost object\n",
    "xgb_model = XGBClassifier(learning_rate =0.01, \n",
    "                      subsample=0.75, \n",
    "                      colsample_bytree=0.72, \n",
    "                      min_child_weight=8,\n",
    "                      max_depth=5)\n",
    "\n",
    "# Train a XGBoost model with Train dataset\n",
    "xgb_model.fit(X_train, Y_train)\n",
    "\n",
    "# Predict the outcome\n",
    "y_hat_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Compute the accuracy score and print it\n",
    "accuracy_xgb = accuracy_score(Y_test, y_hat_xgb)\n",
    "\n",
    "# Print first 8 rows to visualize the prediction.\n",
    "print (\"First few predicted Loan Status values: \", y_hat_xgb[:8], \"\\n\")\n",
    "\n",
    "# Compute accuracy score\n",
    "print (\"Accuracy of XGBoost model: \", accuracy_xgb)\n",
    "\n",
    "# Print confusion matrix of actual and predicted values using Crosstab function\n",
    "pd.crosstab(Y_test, y_hat_xgb, rownames=['Actual'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1 Print XGBoost Classifier Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the XGBoost Model\n",
    "xgb_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.2 To output the Binary Probability using XGBoost Classifier model ..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<< HOME WORK >>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict the binary probability outcome\n",
    "y_hat_xgb_proba = << your code goes here>> \n",
    "\n",
    "# Print first 8 rows to visualize the prediction.\n",
    "y_hat_xgb_proba[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.3 To draw Area Under the Curve (AUC) of XGBoost model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<< HOME WORK: your code goes here >>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='analyze_results'>5. Analyze the model results</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='print_AUC'>Print Accuracy and AUC of all models</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auc_xgb = 0.7253211629479379  # Computed before hand\n",
    "auc_knn = 0.5086206896551724  # Computed before hand\n",
    "\n",
    "# Create a dataframe with Accuracy and AUC\n",
    "acc_auc_df = pd.DataFrame(\n",
    "                    {'Metrics': ['Accuracy', 'AUC'],\n",
    "                    'Logistic Regression': [accuracy_lr, auc_lr],\n",
    "                    'Random Forest': [accuracy_rf, auc_rf],\n",
    "                    'k-NN': [accuracy_knn, auc_knn],\n",
    "                    'SVM': [accuracy_svm, auc_svm],\n",
    "                    'XGBoost': [accuracy_xgb, auc_xgb]}\n",
    ")\n",
    "\n",
    "acc_auc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='k_fold'>6. Split the dataset. _Technique: K-Fold Cross Validation_</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the number of splits\n",
    "NO_SPLITS = 5\n",
    "\n",
    "# Create KFold object with number of splits\n",
    "kf = KFold(n_splits=NO_SPLITS, random_state=111)\n",
    "\n",
    "# Create temp datasets to store the X and  part of the dataset\n",
    "loan_data_X = loan_data.drop('Loan_Status',  axis = 1)\n",
    "loan_data_Y = loan_data['Loan_Status']\n",
    "\n",
    "# loop through the folds and print the shape of them. \n",
    "# NOTE: Printing of the shape is not necessary during production.\n",
    "# This is for illustration purpose only\n",
    "# The below FOR Loop code is not required during production\n",
    "for train_index, test_index in kf.split(loan_data_X):\n",
    "    \n",
    "    # Split train and test datasets sing fold index\n",
    "    X_train, X_test = loan_data_X.iloc[train_index], loan_data_X.iloc[test_index]\n",
    "    Y_train, Y_test = loan_data_Y[train_index], loan_data_Y[test_index]\n",
    "    \n",
    "    # Print the shape of the Train set\n",
    "    print(\"Train dataset: \", X_train.shape, Y_train.shape)\n",
    "\n",
    "    # Print the shape of the Test set\n",
    "    print(\"Test dataset: \", X_test.shape, Y_test.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='binomial_kfold'>7. Generate Binary Classification Models using K-Fold split</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='lr_kfold'>7.1 Logistic Regression Model using K-Fold split</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate a Logistic Regression object\n",
    "lr_KFold_model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Define a variable to store the sum of accuracy of each fold\n",
    "accuracy_score_sum = 0\n",
    "\n",
    "# Split the data using KFold object, run the model iteratively, and compute accuracy\n",
    "for train_index, test_index in kf.split(loan_data_X):\n",
    "\n",
    "    # Split train and test datasets using fold index\n",
    "    X_train, X_test = loan_data_X.iloc[train_index], loan_data_X.iloc[test_index]\n",
    "    Y_train, Y_test = loan_data_Y[train_index], loan_data_Y[test_index]\n",
    "\n",
    "    # Train a Logistic Regression model with Train dataset\n",
    "    lr_KFold_model.fit(X_train, Y_train)\n",
    "\n",
    "    # Compute the accuracy score and print it\n",
    "    # It does two things. First, with 'X_test', it predicts the 'y-hat'. \n",
    "    # Second, with 'y-hat' and 'Y_test' it computes accuracy score\n",
    "    accuracy_score = lr_KFold_model.score(X_test, Y_test)\n",
    "    \n",
    "    # Print accuracy score\n",
    "    print(accuracy_score)\n",
    "    \n",
    "    # Add accuracy of each iterations\n",
    "    accuracy_score_sum += accuracy_score\n",
    "\n",
    "# Compute the mean accuracy score of K-Folds and print it\n",
    "mean_accuracy = accuracy_score_sum / NO_SPLITS\n",
    "print (\"Final accuracy of KFold CV using Logistic Regression: \", mean_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='rf_kfold'>7.2 Random Forest Model using K-Fold split</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate a Random Forest object\n",
    "rf_KFold_model = RandomForestClassifier(n_estimators=100, min_samples_leaf=1, random_state=2202)\n",
    "\n",
    "# Define a variable to store the sum of accuracy of each fold\n",
    "accuracy_score_sum = 0\n",
    "\n",
    "# Create temp datasets to store the X and Y part of the dataset\n",
    "loan_data_X = loan_data.drop('Loan_Status',  axis = 1)\n",
    "loan_data_Y = loan_data['Loan_Status']\n",
    "\n",
    "# Split the data using KFold object, run the model iteratively, and compute accuracy\n",
    "for train_index, test_index in kf.split(loan_data_X):\n",
    "\n",
    "    # Split train and test datasets using fold index\n",
    "    X_train, X_test = loan_data_X.iloc[train_index], loan_data_X.iloc[test_index]\n",
    "    Y_train, Y_test = loan_data_Y[train_index], loan_data_Y[test_index]\n",
    "\n",
    "    # Train a Random Forest model with Train dataset\n",
    "    rf_KFold_model.fit(X_train, Y_train)\n",
    "\n",
    "    # Compute the accuracy score and print it\n",
    "    accuracy_score = rf_KFold_model.score(X_test, Y_test)\n",
    "    \n",
    "    # Print accuracy score\n",
    "    print(accuracy_score)\n",
    "    \n",
    "    # Add accuracy of each iterations\n",
    "    accuracy_score_sum += accuracy_score\n",
    "\n",
    "# Compute the mean accuracy score of K-Folds and print it\n",
    "mean_accuracy = accuracy_score_sum / NO_SPLITS\n",
    "print (\"Final accuracy of KFold CV using Random Forest model: \", mean_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='knn_kfold'>7.3 k-Nearest Neighbor Model using K-Fold split</a>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<< HOME WORK >>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<< Your code goes here.. >>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='svm_kfold'>7.4 Support Vector Machine (SVM) Model using K-Fold split</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate a Support Vector Machine (SVM) object\n",
    "svm_KFold_model = svm.SVC(gamma='scale')\n",
    "\n",
    "# Define a variable to store the sum of accuracy of each fold\n",
    "accuracy_score_sum = 0\n",
    "\n",
    "# Create temp datasets to store the X and Y part of the dataset\n",
    "loan_data_X = loan_data.drop('Loan_Status',  axis = 1)\n",
    "loan_data_Y = loan_data['Loan_Status']\n",
    "\n",
    "# Split the data using KFold object, run the model iteratively, and compute accuracy\n",
    "for train_index, test_index in kf.split(loan_data_X):\n",
    "\n",
    "    # Split train and test datasets using fold index\n",
    "    X_train, X_test = loan_data_X.iloc[train_index], loan_data_X.iloc[test_index]\n",
    "    Y_train, Y_test = loan_data_Y[train_index], loan_data_Y[test_index]\n",
    "\n",
    "    # Train a Random Forest model with Train dataset\n",
    "    svm_KFold_model.fit(X_train, Y_train)\n",
    "\n",
    "    # Compute the accuracy score and print it\n",
    "    accuracy_score = svm_KFold_model.score(X_test, Y_test)\n",
    "    \n",
    "    # Print accuracy score\n",
    "    print(accuracy_score)\n",
    "    \n",
    "    # Add accuracy of each iterations\n",
    "    accuracy_score_sum += accuracy_score\n",
    "\n",
    "# Compute the mean accuracy score of K-Folds and print it\n",
    "mean_accuracy = accuracy_score_sum / NO_SPLITS\n",
    "print (\"Final accuracy of KFold CV using Random Forest model: \", mean_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='xgb_kfold'>7.5 XGBoost Classifier Model using K-Fold split</a>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<< HOME WORK >>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "<< Your code goes here.. >>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
